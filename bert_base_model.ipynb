{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "LYRICS_FILES = '/Users/stamatiosorphanos/Documents/MultiModal-Deep-Learning/MultiModal/lyrics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Get the Bert-base-uncased model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "lyrics_data = []\n",
    "lyrics_files = os.listdir(LYRICS_FILES)\n",
    "\n",
    "# Get the lyrics of each file \n",
    "for file_path in lyrics_files:\n",
    "    if str(file_path).__contains__('.txt'):\n",
    "        with open(os.path.join(LYRICS_FILES, file_path), \"r\") as f:\n",
    "            lyrics = f.read()\n",
    "            lyrics_data.append(lyrics)\n",
    "\n",
    "# Define the tokenizer model\n",
    "encoded_data = tokenizer(lyrics_data, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Create an embedding for the lyrics data based on the last hidden layer \n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_data)\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "\n",
    "# Save the embeddings\n",
    "embedding_file = '/Users/stamatiosorphanos/Documents/MultiModal-Deep-Learning/MultiModal/saved_models/embeddings.pt'\n",
    "torch.save(embeddings, embedding_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model and check for a test song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text similarity function \n",
    "def text_similarity(file_path, embedding_file, top_results):\n",
    "    saved_embeddings = torch.load(embedding_file)\n",
    "    test_lyrics = file_path\n",
    "    \n",
    "    with open(test_lyrics, \"r\") as f:\n",
    "        test_lyrics = f.read()\n",
    "\n",
    "    encoded_test = tokenizer(test_lyrics, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_output = model(**encoded_test)\n",
    "        test_embedding = test_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "    similarity_scores = torch.nn.functional.cosine_similarity(test_embedding, saved_embeddings)\n",
    "    sorted_scores, sorted_indices = similarity_scores.sort(descending=True)\n",
    "\n",
    "    k = top_results\n",
    "    top_k_scores = sorted_scores[:k]\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "\n",
    "    print(\"Top {} similar songs:\".format(k))\n",
    "    for score, index in zip(top_k_scores, top_k_indices):\n",
    "        print(\"Song:\", lyrics_files[index], \"Score:\", score.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 similar songs:\n",
      "Song: Calvin-Harris-Dua-Lipa-One-Kiss.txt Score: 0.9376112818717957\n",
      "Song: The-Weeknd-Out-of-Time.csv Score: 0.9279755353927612\n",
      "Song: Taylor-Swift-Delicate.csv Score: 0.9247044324874878\n",
      "Song: Taylor-Swift-You-Need-To-Calm-Down.csv Score: 0.9153817296028137\n",
      "Song: Imagine-Dragons-Thunder.txt Score: 0.9125005006790161\n",
      "Song: Bruno-Mars-That-s-What-I-Like.txt Score: 0.9101618528366089\n",
      "Song: Conan-Gray-Heather.txt Score: 0.905781090259552\n",
      "Song: M-neskin-Beggin.csv Score: 0.9048627614974976\n",
      "Song: Glass-Animals-Heat-Waves.txt Score: 0.9045715928077698\n",
      "Song: Kygo-Ellie-Goulding-First-Time.txt Score: 0.9004665613174438\n"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/stamatiosorphanos/Documents/MultiModal-Deep-Learning/MultiModal/test.txt'\n",
    "embedding_path = \"/Users/stamatiosorphanos/Documents/MultiModal-Deep-Learning/MultiModal/saved_models/embeddings.pt\"\n",
    "top_results = 10\n",
    "\n",
    "text_similarity(file_path, embedding_path, top_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
